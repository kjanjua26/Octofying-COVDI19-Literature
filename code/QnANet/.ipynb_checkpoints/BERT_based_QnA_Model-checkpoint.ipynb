{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q/A Model for CORD Dataset\n",
    "In this task, we employ finetuned BERT for Q/A scenario on CORD dataset.\n",
    "This notebook is based on:\n",
    "1. https://medium.com/@aakashgoel12/question-answering-system-on-corona-approach-01-6ef9799695cb\n",
    "2. https://medium.com/illuin/unsupervised-question-answering-4758e5f2be9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import json,os,gc\n",
    "basepath = '/Users/Janjua/Desktop/Projects/Octofying-COVID19-Literature/dataset/CORD-19-research-challenge/'\n",
    "path1 = basepath + 'biorxiv_medrxiv/biorxiv_medrxiv/'\n",
    "path2 = basepath + 'comm_use_subset/comm_use_subset/'\n",
    "path3 = basepath + 'custom_license/custom_license/'\n",
    "path4 = basepath + 'noncomm_use_subset/noncomm_use_subset/'\n",
    "paths = [path1,path2,path3,path4]\n",
    "file_names = []\n",
    "for path in paths:\n",
    "    temp_file_names = os.listdir(path)\n",
    "    file_names.extend([path+file_name for file_name in temp_file_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 33375/33375 [10:27<00:00, 53.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "# nltk.download('all')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import os,re,multiprocessing,joblib\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "def file_content(file_path):\n",
    "  abstract='';body_text = '';error_count = 0\n",
    "  if os.path.splitext(file_path)[1]=='.json':\n",
    "    f = open(file_path)\n",
    "    f_json = json.load(f)\n",
    "    try:\n",
    "      abstract = f_json['abstract'][0]['text']\n",
    "    except:\n",
    "      error_count+=1\n",
    "    for i in f_json['body_text']:\n",
    "      try:\n",
    "        body_text= body_text+' '+i['text']\n",
    "      except:\n",
    "        error_count+=1\n",
    "    body_text = body_text.strip()\n",
    "    f.close()\n",
    "    return body_text,abstract,error_count\n",
    "  else:\n",
    "    return body_text,abstract,error_count\n",
    "## Storing article and related information in data-frame\n",
    "df = pd.DataFrame({'file_name':[],'body':[],'abstract':[],'error_count':[]})\n",
    "df['file_name'] = file_names\n",
    "df['article_no'] = list(range(df.shape[0]))\n",
    "for ind,info in tqdm(df.iterrows(),total=df.shape[0]):  df.loc[ind,'body'],df.loc[ind,'abstract'],df.loc[ind,'error_count'] = \\\n",
    "  file_content(file_path=info['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopword.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d1616bd3526d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m## removing stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopword_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopword.txt'"
     ]
    }
   ],
   "source": [
    "corpus_file = 'corpus.txt'\n",
    "sent_dict_file = 'sent.joblib.compressed'\n",
    "word_sent_no_dict_file = 'word_sent_no.joblib.compressed'\n",
    "orig_word_sent_no_dict_file = 'orig_word_sent_no.joblib.compressed'\n",
    "stopword_file = 'stopword.txt'\n",
    "\n",
    "## Lemmatization function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemmatize(sent):\n",
    "  return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(sent)])\n",
    "\n",
    "def parallelize_dataframe(df, func, num_partitions, num_cores):\n",
    "  df_split = np.array_split(df, num_partitions)\n",
    "  pool = Pool(num_cores)\n",
    "  df = pd.concat(pool.map(func, df_split))\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return df\n",
    "\n",
    "def fn_lemmatize(data):\n",
    "  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "    data.loc[ind,'sentence_lemmatized'] = get_lemmatize(sent = info['sentence'])\n",
    "  return data\n",
    "\n",
    "## removing stopwords\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "stopwords = list(set(words(open(stopword_file).read())))\n",
    "\n",
    "def remove_stopwords(sent):\n",
    "  ## case conversion - lower case\n",
    "  word_tokens = words(text=sent)\n",
    "  #sent = sent.lower()\n",
    "  #word_tokens = word_tokenize(sent)\n",
    "  ## removing stopwords\n",
    "  filtered_sentence = \" \".join([w for w in word_tokens if not w in stopwords])\n",
    "  ## removing digits\n",
    "  filtered_sentence = re.sub(r'\\d+','',filtered_sentence)\n",
    "  ## removing multiple space\n",
    "  filtered_sentence = words(text = filtered_sentence)\n",
    "  return \" \".join(filtered_sentence)\n",
    "\n",
    "def fn_stopword(data):\n",
    "  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "    sent = info['sentence_lemmatized']\n",
    "    data.loc[ind,'sentence_lemma_stop'] = remove_stopwords(sent)\n",
    "  return data\n",
    "\n",
    "def fn_stopword_orig(data):\n",
    "  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "    sent = info['sentence']\n",
    "    data.loc[ind,'sentence_stop'] = remove_stopwords(sent)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
